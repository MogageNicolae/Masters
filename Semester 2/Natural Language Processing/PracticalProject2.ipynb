{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-26T19:20:17.135784Z",
     "start_time": "2025-05-26T19:19:44.835240Z"
    }
   },
   "source": "!pip install transformers datasets sacrebleu sentencepiece evaluate sacremoses matplotlib accelerate torch",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\r\n",
      "  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\r\n",
      "Collecting datasets\r\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\r\n",
      "Collecting sacrebleu\r\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\r\n",
      "Collecting sentencepiece\r\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\r\n",
      "Collecting evaluate\r\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\r\n",
      "Collecting sacremoses\r\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\r\n",
      "Collecting matplotlib\r\n",
      "  Downloading matplotlib-3.10.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\r\n",
      "Collecting accelerate\r\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\r\n",
      "Collecting torch\r\n",
      "  Downloading torch-2.7.0-cp311-none-macosx_11_0_arm64.whl.metadata (29 kB)\r\n",
      "Collecting filelock (from transformers)\r\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\r\n",
      "  Downloading huggingface_hub-0.32.1-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting numpy>=1.17 (from transformers)\r\n",
      "  Downloading numpy-2.2.6-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers) (6.0.2)\r\n",
      "Collecting regex!=2019.12.17 (from transformers)\r\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from transformers) (2.32.3)\r\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\r\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\r\n",
      "Collecting safetensors>=0.4.3 (from transformers)\r\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\r\n",
      "Collecting tqdm>=4.27 (from transformers)\r\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\r\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\r\n",
      "  Downloading pyarrow-20.0.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.3 kB)\r\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\r\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting pandas (from datasets)\r\n",
      "  Downloading pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (89 kB)\r\n",
      "Collecting xxhash (from datasets)\r\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\r\n",
      "Collecting multiprocess<0.70.17 (from datasets)\r\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\r\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\r\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting portalocker (from sacrebleu)\r\n",
      "  Using cached portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\r\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\r\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\r\n",
      "Collecting colorama (from sacrebleu)\r\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\r\n",
      "Collecting lxml (from sacrebleu)\r\n",
      "  Downloading lxml-5.4.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.5 kB)\r\n",
      "Collecting click (from sacremoses)\r\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting joblib (from sacremoses)\r\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\r\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\r\n",
      "  Downloading contourpy-1.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.5 kB)\r\n",
      "Collecting cycler>=0.10 (from matplotlib)\r\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\r\n",
      "  Downloading fonttools-4.58.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (104 kB)\r\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\r\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.2 kB)\r\n",
      "Collecting pillow>=8 (from matplotlib)\r\n",
      "  Downloading pillow-11.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.9 kB)\r\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\r\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from accelerate) (7.0.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.11/site-packages (from torch) (4.13.2)\r\n",
      "Collecting sympy>=1.13.3 (from torch)\r\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting networkx (from torch)\r\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.6)\r\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\r\n",
      "  Downloading aiohttp-3.12.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.6 kB)\r\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.30.0->transformers)\r\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2025.4.26)\r\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\r\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\r\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\r\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\r\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\r\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\r\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\r\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\r\n",
      "  Downloading frozenlist-1.6.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (16 kB)\r\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\r\n",
      "  Downloading multidict-6.4.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.3 kB)\r\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\r\n",
      "  Downloading propcache-0.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (10 kB)\r\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\r\n",
      "  Downloading yarl-1.20.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (72 kB)\r\n",
      "Downloading transformers-4.52.3-py3-none-any.whl (10.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/10.5 MB\u001B[0m \u001B[31m51.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\r\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\r\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-macosx_11_0_arm64.whl (1.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m75.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\r\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m897.5/897.5 kB\u001B[0m \u001B[31m65.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading matplotlib-3.10.3-cp311-cp311-macosx_11_0_arm64.whl (8.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.1/8.1 MB\u001B[0m \u001B[31m72.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\r\n",
      "Downloading torch-2.7.0-cp311-none-macosx_11_0_arm64.whl (68.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m68.6/68.6 MB\u001B[0m \u001B[31m77.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading contourpy-1.3.2-cp311-cp311-macosx_11_0_arm64.whl (254 kB)\r\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\r\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\r\n",
      "Downloading fonttools-4.58.0-cp311-cp311-macosx_10_9_universal2.whl (2.8 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.8/2.8 MB\u001B[0m \u001B[31m61.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\r\n",
      "Downloading huggingface_hub-0.32.1-py3-none-any.whl (509 kB)\r\n",
      "Downloading kiwisolver-1.4.8-cp311-cp311-macosx_11_0_arm64.whl (65 kB)\r\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\r\n",
      "Downloading numpy-2.2.6-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.4/5.4 MB\u001B[0m \u001B[31m76.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading pillow-11.2.1-cp311-cp311-macosx_11_0_arm64.whl (3.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m70.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading pyarrow-20.0.0-cp311-cp311-macosx_12_0_arm64.whl (30.9 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m30.9/30.9 MB\u001B[0m \u001B[31m83.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\r\n",
      "Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\r\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\r\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\r\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\r\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\r\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\r\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\r\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\r\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\r\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\r\n",
      "Downloading lxml-5.4.0-cp311-cp311-macosx_10_9_universal2.whl (8.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.1/8.1 MB\u001B[0m \u001B[31m51.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hUsing cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\r\n",
      "Downloading pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.3/11.3 MB\u001B[0m \u001B[31m80.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hUsing cached portalocker-3.1.1-py3-none-any.whl (19 kB)\r\n",
      "Downloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\r\n",
      "Downloading aiohttp-3.12.1-cp311-cp311-macosx_11_0_arm64.whl (462 kB)\r\n",
      "Downloading hf_xet-1.1.2-cp37-abi3-macosx_11_0_arm64.whl (2.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m76.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\r\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\r\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\r\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\r\n",
      "Downloading frozenlist-1.6.0-cp311-cp311-macosx_11_0_arm64.whl (122 kB)\r\n",
      "Downloading multidict-6.4.4-cp311-cp311-macosx_11_0_arm64.whl (37 kB)\r\n",
      "Downloading propcache-0.3.1-cp311-cp311-macosx_11_0_arm64.whl (45 kB)\r\n",
      "Downloading yarl-1.20.0-cp311-cp311-macosx_11_0_arm64.whl (94 kB)\r\n",
      "Installing collected packages: sentencepiece, pytz, mpmath, xxhash, tzdata, tqdm, tabulate, sympy, safetensors, regex, pyparsing, pyarrow, propcache, portalocker, pillow, numpy, networkx, multidict, lxml, kiwisolver, joblib, hf-xet, fsspec, frozenlist, fonttools, filelock, dill, cycler, colorama, click, aiohappyeyeballs, yarl, torch, sacremoses, sacrebleu, pandas, multiprocess, huggingface-hub, contourpy, aiosignal, tokenizers, matplotlib, aiohttp, accelerate, transformers, datasets, evaluate\r\n",
      "Successfully installed accelerate-1.7.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.1 aiosignal-1.3.2 click-8.2.1 colorama-0.4.6 contourpy-1.3.2 cycler-0.12.1 datasets-3.6.0 dill-0.3.8 evaluate-0.4.3 filelock-3.18.0 fonttools-4.58.0 frozenlist-1.6.0 fsspec-2025.3.0 hf-xet-1.1.2 huggingface-hub-0.32.1 joblib-1.5.1 kiwisolver-1.4.8 lxml-5.4.0 matplotlib-3.10.3 mpmath-1.3.0 multidict-6.4.4 multiprocess-0.70.16 networkx-3.4.2 numpy-2.2.6 pandas-2.2.3 pillow-11.2.1 portalocker-3.1.1 propcache-0.3.1 pyarrow-20.0.0 pyparsing-3.2.3 pytz-2025.2 regex-2024.11.6 sacrebleu-2.5.1 sacremoses-0.1.1 safetensors-0.5.3 sentencepiece-0.2.0 sympy-1.14.0 tabulate-0.9.0 tokenizers-0.21.1 torch-2.7.0 tqdm-4.67.1 transformers-4.52.3 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T19:20:31.927092Z",
     "start_time": "2025-05-26T19:20:17.148260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    MT5Tokenizer,\n",
    "    MT5ForConditionalGeneration\n",
    ")\n",
    "from datasets import load_dataset, Dataset as HFDataset, concatenate_datasets\n",
    "import evaluate\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ],
   "id": "1c11ce6b536fa737",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolae.mogage/Uni/Masters/Semester 2/Natural Language Processing/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T19:20:32.024788Z",
     "start_time": "2025-05-26T19:20:32.021579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import islice\n",
    "\n",
    "LANGS_NLLB = {\n",
    "    \"en\": \"eng_Latn\",\n",
    "    \"fr\": \"fra_Latn\",\n",
    "    \"it\": \"ita_Latn\",\n",
    "    \"de\": \"deu_Latn\",\n",
    "    \"es\": \"spa_Latn\",\n",
    "    \"ro\": \"ron_Latn\"\n",
    "}\n",
    "\n",
    "def load_and_limit_dataset(dataset_name, config, limit=25_000, streaming=True, field=\"translation\"):\n",
    "    if isinstance(config, dict):\n",
    "        dataset = load_dataset(\n",
    "            dataset_name,\n",
    "            streaming=streaming,\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True,\n",
    "            **config,\n",
    "        )\n",
    "    else:\n",
    "        dataset = load_dataset(\n",
    "            dataset_name,\n",
    "            config,\n",
    "            streaming=streaming,\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "    dataset = dataset.remove_columns([c for c in dataset.column_names if c != field])\n",
    "    limited = list(islice(dataset, limit))\n",
    "    return HFDataset.from_list(limited)\n",
    "\n",
    "def load_and_limit_tatoeba(src_lang, tgt_lang=\"ro\", limit=25_000):\n",
    "    return load_and_limit_dataset(\n",
    "        \"tatoeba\",\n",
    "        {\n",
    "            \"lang1\": src_lang,\n",
    "            \"lang2\": tgt_lang\n",
    "        },\n",
    "        limit=limit,\n",
    "        streaming=False,\n",
    "    )\n",
    "\n",
    "def load_and_limit_open_subtitles(src_lang, tgt_lang=\"ro\", limit=25_000):\n",
    "    return load_and_limit_dataset(\n",
    "        \"open_subtitles\",\n",
    "        {\n",
    "            \"lang1\": src_lang,\n",
    "            \"lang2\": tgt_lang\n",
    "        },\n",
    "        limit=limit,\n",
    "        streaming=True,\n",
    "    )\n",
    "\n",
    "def load_and_limit_nllb(src_lang, tgt_lang=\"ro\", limit=25_000):\n",
    "    src_lang = LANGS_NLLB[src_lang]\n",
    "    tgt_lang = LANGS_NLLB[tgt_lang]\n",
    "\n",
    "    return load_and_limit_dataset(\n",
    "        \"allenai/nllb\",\n",
    "        f\"{src_lang}-{tgt_lang}\",\n",
    "        limit=limit,\n",
    "        streaming=True,\n",
    "    )\n",
    "\n",
    "def load_and_limit_ccmatrix(src_lang, tgt_lang=\"ro\", limit=25_000):\n",
    "    return load_and_limit_dataset(\n",
    "        \"yhavinga/ccmatrix\",\n",
    "        f\"{src_lang}-{tgt_lang}\",\n",
    "        limit=limit,\n",
    "        streaming=True,\n",
    "    )"
   ],
   "id": "29dc4aafd9e4a52f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T19:21:50.510234Z",
     "start_time": "2025-05-26T19:20:32.031109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reformat(dataset, src_lang, is_nllb=False):\n",
    "    tgt_lang = \"ro\"\n",
    "    if is_nllb:\n",
    "        src_lang = LANGS_NLLB[src_lang]\n",
    "        tgt_lang = LANGS_NLLB[tgt_lang]\n",
    "\n",
    "    def map_fn(example):\n",
    "        return {\n",
    "            \"src\": f'<{src_lang}> {example[\"translation\"][src_lang]}',\n",
    "            \"trg\": example[\"translation\"][tgt_lang]\n",
    "        }\n",
    "    return dataset.map(map_fn, remove_columns=dataset.column_names)\n",
    "\n",
    "src_langs = [\"en\", \"fr\", \"it\", \"de\", \"es\"]\n",
    "limit = 5_000\n",
    "\n",
    "processed_ds = []\n",
    "for lg in src_langs:\n",
    "    ccmatrix = load_and_limit_ccmatrix(lg, limit=limit)\n",
    "    tatoeba = load_and_limit_tatoeba(lg, limit=limit)\n",
    "    open_subtitles = load_and_limit_open_subtitles(lg, limit=limit)\n",
    "    nllb = load_and_limit_nllb(lg, limit=limit)\n",
    "\n",
    "    processed_ds.append(reformat(ccmatrix, lg))\n",
    "    processed_ds.append(reformat(tatoeba, lg))\n",
    "    processed_ds.append(reformat(open_subtitles, lg))\n",
    "    processed_ds.append(reformat(nllb, lg, True))\n",
    "\n",
    "combined_dataset = concatenate_datasets(processed_ds)"
   ],
   "id": "8f2c42d986c75160",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 514k/514k [00:00<00:00, 1.72MB/s]\n",
      "Generating train split: 15182 examples [00:00, 81647.78 examples/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 72813.23 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 78167.65 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 81113.62 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 75602.74 examples/s]\n",
      "Downloading data: 100%|██████████| 69.0k/69.0k [00:00<00:00, 1.16MB/s]\n",
      "Generating train split: 1965 examples [00:00, 99619.35 examples/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 81838.48 examples/s]\n",
      "Map: 100%|██████████| 1965/1965 [00:00<00:00, 66261.53 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 81097.62 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 80272.22 examples/s]\n",
      "Downloading data: 100%|██████████| 37.5k/37.5k [00:00<00:00, 629kB/s]\n",
      "Generating train split: 1018 examples [00:00, 101962.97 examples/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 33189.30 examples/s]\n",
      "Map: 100%|██████████| 1018/1018 [00:00<00:00, 15338.47 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 36582.59 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 14647.68 examples/s]\n",
      "Downloading data: 100%|██████████| 77.1k/77.1k [00:00<00:00, 1.30MB/s]\n",
      "Generating train split: 2160 examples [00:00, 98418.27 examples/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 52497.97 examples/s]\n",
      "Map: 100%|██████████| 2160/2160 [00:00<00:00, 23010.51 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 29050.53 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 29476.99 examples/s]\n",
      "Downloading data: 100%|██████████| 69.7k/69.7k [00:00<00:00, 588kB/s]\n",
      "Generating train split: 1970 examples [00:00, 115576.27 examples/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 77164.16 examples/s]\n",
      "Map: 100%|██████████| 1970/1970 [00:00<00:00, 65756.61 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 81724.63 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 76123.05 examples/s]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T19:21:50.528131Z",
     "start_time": "2025-05-26T19:21:50.526124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(combined_dataset))\n",
    "print(combined_dataset[0])"
   ],
   "id": "fb1ac099f76337cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87113\n",
      "{'src': '<en> We might call them the words of \"unforgiveness.\"', 'trg': 'Le putem numi cuvintele „ne-iertării”.'}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T19:21:50.580264Z",
     "start_time": "2025-05-26T19:21:50.556072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "shuffled_dataset = combined_dataset.shuffle(seed=42)\n",
    "\n",
    "train_val_split = combined_dataset.train_test_split(test_size=0.20, seed=42)\n",
    "train_val = train_val_split[\"train\"]\n",
    "test_hf_dataset = train_val_split[\"test\"]\n",
    "\n",
    "train_val_split_2 = train_val.train_test_split(test_size=0.25, seed=42)\n",
    "train_hf_dataset = train_val_split_2[\"train\"]\n",
    "val_hf_dataset = train_val_split_2[\"test\"]\n",
    "\n",
    "print(f\"Train size: {len(train_hf_dataset)}\")\n",
    "print(f\"Validation size: {len(val_hf_dataset)}\")\n",
    "print(f\"Test size: {len(test_hf_dataset)}\")\n"
   ],
   "id": "40f84c0f5c1b0f16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 49000\n",
      "Validation size: 16334\n",
      "Test size: 21779\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T19:22:09.763132Z",
     "start_time": "2025-05-26T19:21:50.586820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"google/mt5-small\"\n",
    "\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.to(device)"
   ],
   "id": "f8a873e2cad7ed98",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MT5ForConditionalGeneration(\n",
       "  (shared): Embedding(250112, 512)\n",
       "  (encoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250112, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250112, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T19:22:19.575841Z",
     "start_time": "2025-05-26T19:22:09.780901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"src\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"trg\"],\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train_dataset = train_hf_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val_dataset = val_hf_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test_dataset = test_hf_dataset.map(preprocess_function, batched=True)"
   ],
   "id": "c9f8d7ad0c9f1f61",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/49000 [00:00<?, ? examples/s]/Users/nicolae.mogage/Uni/Masters/Semester 2/Natural Language Processing/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3959: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 49000/49000 [00:05<00:00, 8922.03 examples/s]\n",
      "Map: 100%|██████████| 16334/16334 [00:01<00:00, 8611.72 examples/s]\n",
      "Map: 100%|██████████| 21779/21779 [00:02<00:00, 9175.10 examples/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T19:22:21.957042Z",
     "start_time": "2025-05-26T19:22:19.583654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mt5_translation\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "metric = evaluate.load(\"bleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in labels (default ignore index)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=[[label] for label in decoded_labels]\n",
    "    )\n",
    "    return {\"bleu\": result[\"bleu\"]}\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "1b7354b643a1431e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 5.94k/5.94k [00:00<00:00, 11.0MB/s]\n",
      "Downloading extra modules: 4.07kB [00:00, 1.70MB/s]                   \n",
      "Downloading extra modules: 100%|██████████| 3.34k/3.34k [00:00<00:00, 8.29MB/s]\n",
      "/var/folders/gg/xybng_gj2rz3ltl94z9r_c59xzqsfn/T/ipykernel_30411/1011268881.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T19:23:27.127323Z",
     "start_time": "2025-05-26T19:22:21.971490Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "b3a1b58b850b00f6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolae.mogage/Uni/Masters/Semester 2/Natural Language Processing/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='1532' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  14/1532 00:51 < 1:48:19, 0.23 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Uni/Masters/Semester 2/Natural Language Processing/.venv/lib/python3.11/site-packages/transformers/trainer.py:2240\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2238\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2239\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2240\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2241\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2242\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2243\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2244\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2245\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Uni/Masters/Semester 2/Natural Language Processing/.venv/lib/python3.11/site-packages/transformers/trainer.py:2555\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2548\u001B[39m context = (\n\u001B[32m   2549\u001B[39m     functools.partial(\u001B[38;5;28mself\u001B[39m.accelerator.no_sync, model=model)\n\u001B[32m   2550\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m i != \u001B[38;5;28mlen\u001B[39m(batch_samples) - \u001B[32m1\u001B[39m\n\u001B[32m   2551\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001B[32m   2552\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m contextlib.nullcontext\n\u001B[32m   2553\u001B[39m )\n\u001B[32m   2554\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m-> \u001B[39m\u001B[32m2555\u001B[39m     tr_loss_step = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2557\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2558\u001B[39m     args.logging_nan_inf_filter\n\u001B[32m   2559\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[32m   2560\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (torch.isnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch.isinf(tr_loss_step))\n\u001B[32m   2561\u001B[39m ):\n\u001B[32m   2562\u001B[39m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[32m   2563\u001B[39m     tr_loss = tr_loss + tr_loss / (\u001B[32m1\u001B[39m + \u001B[38;5;28mself\u001B[39m.state.global_step - \u001B[38;5;28mself\u001B[39m._globalstep_last_logged)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Uni/Masters/Semester 2/Natural Language Processing/.venv/lib/python3.11/site-packages/transformers/trainer.py:3791\u001B[39m, in \u001B[36mTrainer.training_step\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m   3788\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001B[32m   3789\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mscale_wrt_gas\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3791\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43maccelerator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3793\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss.detach()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Uni/Masters/Semester 2/Natural Language Processing/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:2473\u001B[39m, in \u001B[36mAccelerator.backward\u001B[39m\u001B[34m(self, loss, **kwargs)\u001B[39m\n\u001B[32m   2471\u001B[39m     \u001B[38;5;28mself\u001B[39m.lomo_backward(loss, learning_rate)\n\u001B[32m   2472\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2473\u001B[39m     \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Uni/Masters/Semester 2/Natural Language Processing/.venv/lib/python3.11/site-packages/torch/_tensor.py:648\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    640\u001B[39m         Tensor.backward,\n\u001B[32m    641\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    646\u001B[39m         inputs=inputs,\n\u001B[32m    647\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Uni/Masters/Semester 2/Natural Language Processing/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    348\u001B[39m     retain_graph = create_graph\n\u001B[32m    350\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Uni/Masters/Semester 2/Natural Language Processing/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    822\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    823\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m824\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    825\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    826\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    827\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    828\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.evaluate()",
   "id": "8a2aed09ae43156e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
