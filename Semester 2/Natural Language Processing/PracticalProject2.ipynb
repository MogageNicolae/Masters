{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-26T19:09:07.622661Z",
     "start_time": "2025-05-26T19:09:06.347417Z"
    }
   },
   "source": "!pip install transformers datasets sacrebleu sentencepiece evaluate sacremoses matplotlib accelerate torch",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: datasets in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: sacrebleu in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: sentencepiece in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: evaluate in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: sacremoses in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: matplotlib in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: accelerate in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: filelock in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from transformers) (0.32.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.1)\n",
      "Requirement already satisfied: portalocker in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from sacrebleu) (3.1.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from sacrebleu) (5.4.0)\n",
      "Requirement already satisfied: click in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from sacremoses) (8.2.1)\n",
      "Requirement already satisfied: joblib in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from sacremoses) (1.5.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: psutil in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from accelerate) (2.7.0+cu126)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: six>=1.5 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: pywin32>=226 in d:\\projects\\masters\\semester 2\\natural language processing\\.venv\\lib\\site-packages (from portalocker->sacrebleu) (310)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T19:09:11.500516Z",
     "start_time": "2025-05-26T19:09:07.747225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    MT5Tokenizer,\n",
    "    MT5ForConditionalGeneration\n",
    ")\n",
    "from datasets import load_dataset, Dataset as HFDataset, concatenate_datasets\n",
    "import evaluate\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ],
   "id": "1c11ce6b536fa737",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\Masters\\Semester 2\\Natural Language Processing\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T19:09:11.519571Z",
     "start_time": "2025-05-26T19:09:11.514534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import islice\n",
    "\n",
    "LANGS_NLLB = {\n",
    "    \"en\": \"eng_Latn\",\n",
    "    \"fr\": \"fra_Latn\",\n",
    "    \"it\": \"ita_Latn\",\n",
    "    \"de\": \"deu_Latn\",\n",
    "    \"es\": \"spa_Latn\",\n",
    "    \"ro\": \"ron_Latn\"\n",
    "}\n",
    "\n",
    "def load_and_limit_dataset(dataset_name, config, limit=25_000, streaming=True, field=\"translation\"):\n",
    "    if isinstance(config, dict):\n",
    "        dataset = load_dataset(\n",
    "            dataset_name,\n",
    "            streaming=streaming,\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True,\n",
    "            **config,\n",
    "        )\n",
    "    else:\n",
    "        dataset = load_dataset(\n",
    "            dataset_name,\n",
    "            config,\n",
    "            streaming=streaming,\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "    dataset = dataset.remove_columns([c for c in dataset.column_names if c != field])\n",
    "    limited = list(islice(dataset, limit))\n",
    "    return HFDataset.from_list(limited)\n",
    "\n",
    "def load_and_limit_tatoeba(src_lang, tgt_lang=\"ro\", limit=25_000):\n",
    "    return load_and_limit_dataset(\n",
    "        \"tatoeba\",\n",
    "        {\n",
    "            \"lang1\": src_lang,\n",
    "            \"lang2\": tgt_lang\n",
    "        },\n",
    "        limit=limit,\n",
    "        streaming=False,\n",
    "    )\n",
    "\n",
    "def load_and_limit_open_subtitles(src_lang, tgt_lang=\"ro\", limit=25_000):\n",
    "    return load_and_limit_dataset(\n",
    "        \"open_subtitles\",\n",
    "        {\n",
    "            \"lang1\": src_lang,\n",
    "            \"lang2\": tgt_lang\n",
    "        },\n",
    "        limit=limit,\n",
    "        streaming=True,\n",
    "    )\n",
    "\n",
    "def load_and_limit_nllb(src_lang, tgt_lang=\"ro\", limit=25_000):\n",
    "    src_lang = LANGS_NLLB[src_lang]\n",
    "    tgt_lang = LANGS_NLLB[tgt_lang]\n",
    "\n",
    "    return load_and_limit_dataset(\n",
    "        \"allenai/nllb\",\n",
    "        f\"{src_lang}-{tgt_lang}\",\n",
    "        limit=limit,\n",
    "        streaming=True,\n",
    "    )\n",
    "\n",
    "def load_and_limit_ccmatrix(src_lang, tgt_lang=\"ro\", limit=25_000):\n",
    "    return load_and_limit_dataset(\n",
    "        \"yhavinga/ccmatrix\",\n",
    "        f\"{src_lang}-{tgt_lang}\",\n",
    "        limit=limit,\n",
    "        streaming=True,\n",
    "    )"
   ],
   "id": "29dc4aafd9e4a52f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T19:10:26.289082Z",
     "start_time": "2025-05-26T19:09:11.526815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reformat(dataset, src_lang, is_nllb=False):\n",
    "    tgt_lang = \"ro\"\n",
    "    if is_nllb:\n",
    "        src_lang = LANGS_NLLB[src_lang]\n",
    "        tgt_lang = LANGS_NLLB[tgt_lang]\n",
    "\n",
    "    def map_fn(example):\n",
    "        return {\n",
    "            \"src\": f'<{src_lang}> {example[\"translation\"][src_lang]}',\n",
    "            \"trg\": example[\"translation\"][tgt_lang]\n",
    "        }\n",
    "    return dataset.map(map_fn, remove_columns=dataset.column_names)\n",
    "\n",
    "src_langs = [\"en\", \"fr\", \"it\", \"de\", \"es\"]\n",
    "limit = 5_000\n",
    "\n",
    "processed_ds = []\n",
    "for lg in src_langs:\n",
    "    ccmatrix = load_and_limit_ccmatrix(lg, limit=limit)\n",
    "    tatoeba = load_and_limit_tatoeba(lg, limit=limit)\n",
    "    open_subtitles = load_and_limit_open_subtitles(lg, limit=limit)\n",
    "    nllb = load_and_limit_nllb(lg, limit=limit)\n",
    "\n",
    "    processed_ds.append(reformat(ccmatrix, lg))\n",
    "    processed_ds.append(reformat(tatoeba, lg))\n",
    "    processed_ds.append(reformat(open_subtitles, lg))\n",
    "    processed_ds.append(reformat(nllb, lg, True))\n",
    "\n",
    "combined_dataset = concatenate_datasets(processed_ds)"
   ],
   "id": "8f2c42d986c75160",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 63004.78 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 63412.47 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 62959.20 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 61699.45 examples/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 60485.29 examples/s]\n",
      "Map: 100%|██████████| 1965/1965 [00:00<00:00, 62276.11 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 63543.32 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 61621.68 examples/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 60589.44 examples/s]\n",
      "Map: 100%|██████████| 1018/1018 [00:00<00:00, 54130.34 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 64421.51 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 65248.90 examples/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 60879.30 examples/s]\n",
      "Map: 100%|██████████| 2160/2160 [00:00<00:00, 64003.51 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 61230.00 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 61898.86 examples/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 61522.97 examples/s]\n",
      "Map: 100%|██████████| 1970/1970 [00:00<00:00, 63239.26 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 65259.26 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 62399.82 examples/s]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T19:10:26.304636Z",
     "start_time": "2025-05-26T19:10:26.301597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(combined_dataset))\n",
    "print(combined_dataset[0])"
   ],
   "id": "fb1ac099f76337cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87113\n",
      "{'src': '<en> We might call them the words of \"unforgiveness.\"', 'trg': 'Le putem numi cuvintele „ne-iertării”.'}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T19:10:26.388334Z",
     "start_time": "2025-05-26T19:10:26.354767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "shuffled_dataset = combined_dataset.shuffle(seed=42)\n",
    "\n",
    "train_val_split = combined_dataset.train_test_split(test_size=0.25, seed=42)\n",
    "train_val = train_val_split[\"train\"]\n",
    "test_hf_dataset = train_val_split[\"test\"]\n",
    "\n",
    "train_val_split_2 = train_val.train_test_split(test_size=0.25, seed=42)\n",
    "train_hf_dataset = train_val_split_2[\"train\"]\n",
    "val_hf_dataset = train_val_split_2[\"test\"]\n",
    "\n",
    "print(f\"Train size: {len(train_hf_dataset)}\")\n",
    "print(f\"Validation size: {len(val_hf_dataset)}\")\n",
    "print(f\"Test size: {len(test_hf_dataset)}\")\n"
   ],
   "id": "40f84c0f5c1b0f16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 49000\n",
      "Validation size: 16334\n",
      "Test size: 21779\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T19:10:29.586863Z",
     "start_time": "2025-05-26T19:10:26.404588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"google/mt5-small\"\n",
    "\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.to(device)"
   ],
   "id": "f8a873e2cad7ed98",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MT5ForConditionalGeneration(\n",
       "  (shared): Embedding(250112, 512)\n",
       "  (encoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250112, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250112, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T19:10:41.527932Z",
     "start_time": "2025-05-26T19:10:29.598913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"src\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"trg\"],\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train_dataset = train_hf_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val_dataset = val_hf_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test_dataset = test_hf_dataset.map(preprocess_function, batched=True)"
   ],
   "id": "c9f8d7ad0c9f1f61",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/49000 [00:00<?, ? examples/s]D:\\Projects\\Masters\\Semester 2\\Natural Language Processing\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3959: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 49000/49000 [00:06<00:00, 7314.44 examples/s]\n",
      "Map: 100%|██████████| 16334/16334 [00:02<00:00, 7637.10 examples/s]\n",
      "Map: 100%|██████████| 21779/21779 [00:03<00:00, 7178.15 examples/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T19:12:44.497131Z",
     "start_time": "2025-05-26T19:12:42.818244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mt5_translation\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "metric = evaluate.load(\"bleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in labels (default ignore index)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=[[label] for label in decoded_labels]\n",
    "    )\n",
    "    return {\"bleu\": result[\"bleu\"]}\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "1b7354b643a1431e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\AppData\\Local\\Temp\\ipykernel_23616\\1011268881.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T19:12:47.743160Z",
     "start_time": "2025-05-26T19:12:44.501168Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "b3a1b58b850b00f6",
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.82 GiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 25.02 GiB is allocated by PyTorch, and 123.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOutOfMemoryError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\Masters\\Semester 2\\Natural Language Processing\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2240\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2238\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2239\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2240\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2241\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2242\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2243\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2244\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2245\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\Masters\\Semester 2\\Natural Language Processing\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2555\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2548\u001B[39m context = (\n\u001B[32m   2549\u001B[39m     functools.partial(\u001B[38;5;28mself\u001B[39m.accelerator.no_sync, model=model)\n\u001B[32m   2550\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m i != \u001B[38;5;28mlen\u001B[39m(batch_samples) - \u001B[32m1\u001B[39m\n\u001B[32m   2551\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001B[32m   2552\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m contextlib.nullcontext\n\u001B[32m   2553\u001B[39m )\n\u001B[32m   2554\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m-> \u001B[39m\u001B[32m2555\u001B[39m     tr_loss_step = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2557\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2558\u001B[39m     args.logging_nan_inf_filter\n\u001B[32m   2559\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[32m   2560\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (torch.isnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch.isinf(tr_loss_step))\n\u001B[32m   2561\u001B[39m ):\n\u001B[32m   2562\u001B[39m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[32m   2563\u001B[39m     tr_loss = tr_loss + tr_loss / (\u001B[32m1\u001B[39m + \u001B[38;5;28mself\u001B[39m.state.global_step - \u001B[38;5;28mself\u001B[39m._globalstep_last_logged)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\Masters\\Semester 2\\Natural Language Processing\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3745\u001B[39m, in \u001B[36mTrainer.training_step\u001B[39m\u001B[34m(self, model, inputs, num_items_in_batch)\u001B[39m\n\u001B[32m   3742\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb.reduce_mean().detach().to(\u001B[38;5;28mself\u001B[39m.args.device)\n\u001B[32m   3744\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.compute_loss_context_manager():\n\u001B[32m-> \u001B[39m\u001B[32m3745\u001B[39m     loss = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3747\u001B[39m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[32m   3748\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   3749\u001B[39m     \u001B[38;5;28mself\u001B[39m.args.torch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   3750\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.state.global_step % \u001B[38;5;28mself\u001B[39m.args.torch_empty_cache_steps == \u001B[32m0\u001B[39m\n\u001B[32m   3751\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\Masters\\Semester 2\\Natural Language Processing\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3810\u001B[39m, in \u001B[36mTrainer.compute_loss\u001B[39m\u001B[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[39m\n\u001B[32m   3808\u001B[39m         loss_kwargs[\u001B[33m\"\u001B[39m\u001B[33mnum_items_in_batch\u001B[39m\u001B[33m\"\u001B[39m] = num_items_in_batch\n\u001B[32m   3809\u001B[39m     inputs = {**inputs, **loss_kwargs}\n\u001B[32m-> \u001B[39m\u001B[32m3810\u001B[39m outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3811\u001B[39m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[32m   3812\u001B[39m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[32m   3813\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.past_index >= \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\Masters\\Semester 2\\Natural Language Processing\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\Masters\\Semester 2\\Natural Language Processing\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\Masters\\Semester 2\\Natural Language Processing\\.venv\\Lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:1874\u001B[39m, in \u001B[36mMT5ForConditionalGeneration.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[39m\n\u001B[32m   1872\u001B[39m     \u001B[38;5;66;03m# move labels to correct device to enable PP\u001B[39;00m\n\u001B[32m   1873\u001B[39m     labels = labels.to(lm_logits.device)\n\u001B[32m-> \u001B[39m\u001B[32m1874\u001B[39m     loss = \u001B[43mloss_fct\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlm_logits\u001B[49m\u001B[43m.\u001B[49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_logits\u001B[49m\u001B[43m.\u001B[49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m.\u001B[49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1875\u001B[39m     \u001B[38;5;66;03m# TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\u001B[39;00m\n\u001B[32m   1877\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_dict:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\Masters\\Semester 2\\Natural Language Processing\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\Masters\\Semester 2\\Natural Language Processing\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\Masters\\Semester 2\\Natural Language Processing\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1297\u001B[39m, in \u001B[36mCrossEntropyLoss.forward\u001B[39m\u001B[34m(self, input, target)\u001B[39m\n\u001B[32m   1296\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) -> Tensor:\n\u001B[32m-> \u001B[39m\u001B[32m1297\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1298\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1299\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1300\u001B[39m \u001B[43m        \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1301\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1302\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1303\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1304\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\Masters\\Semester 2\\Natural Language Processing\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001B[39m, in \u001B[36mcross_entropy\u001B[39m\u001B[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[39m\n\u001B[32m   3492\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   3493\u001B[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001B[32m-> \u001B[39m\u001B[32m3494\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_C\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_nn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3495\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   3496\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3497\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3498\u001B[39m \u001B[43m    \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3499\u001B[39m \u001B[43m    \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3500\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3501\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mOutOfMemoryError\u001B[39m: CUDA out of memory. Tried to allocate 3.82 GiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 25.02 GiB is allocated by PyTorch, and 123.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.evaluate()",
   "id": "8a2aed09ae43156e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
