{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-04T11:18:42.292083Z",
     "start_time": "2024-12-04T11:18:41.807911Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import vit_b_16, ViT_L_16_Weights, ViT_H_14_Weights"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T15:04:18.637513Z",
     "start_time": "2024-12-03T15:04:18.601464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ],
   "id": "2ea979419e99b497",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T15:20:56.600672Z",
     "start_time": "2024-12-03T15:20:56.034929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ],
   "id": "931e6202ec4da541",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T15:35:17.491996Z",
     "start_time": "2024-12-03T15:35:17.486415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_and_evaluate(model, epochs=5):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.3)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        # scheduler.step()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ],
   "id": "57fe4ab6358eef3f",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T15:20:58.909324Z",
     "start_time": "2024-12-03T15:20:58.802452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten input\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Train and evaluate\n",
    "model = LinearClassifier()\n",
    "train_and_evaluate(model, 10)"
   ],
   "id": "e98c298eafaae17e",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 784]' is invalid for input of size 196608",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Train and evaluate\u001B[39;00m\n\u001B[1;32m     12\u001B[0m model \u001B[38;5;241m=\u001B[39m LinearClassifier()\n\u001B[0;32m---> 13\u001B[0m \u001B[43mtrain_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[6], line 14\u001B[0m, in \u001B[0;36mtrain_and_evaluate\u001B[0;34m(model, epochs)\u001B[0m\n\u001B[1;32m     12\u001B[0m images, labels \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     13\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 14\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[1;32m     16\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/Uni/Masters/Semester 1/Advanced Methods in Data Analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Uni/Masters/Semester 1/Advanced Methods in Data Analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[9], line 7\u001B[0m, in \u001B[0;36mLinearClassifier.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m----> 7\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m28\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m28\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Flatten input\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(x)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: shape '[-1, 784]' is invalid for input of size 196608"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:07:09.269289Z",
     "start_time": "2024-12-03T14:05:46.224208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "# Train and evaluate\n",
    "model = CNN()\n",
    "train_and_evaluate(model, 10)"
   ],
   "id": "536f4dfbad221139",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1672\n",
      "Epoch 2, Loss: 0.0482\n",
      "Epoch 3, Loss: 0.0335\n",
      "Epoch 4, Loss: 0.0229\n",
      "Epoch 5, Loss: 0.0180\n",
      "Epoch 6, Loss: 0.0148\n",
      "Epoch 7, Loss: 0.0048\n",
      "Epoch 8, Loss: 0.0022\n",
      "Epoch 9, Loss: 0.0019\n",
      "Epoch 10, Loss: 0.0012\n",
      "Accuracy: 99.27%\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T15:33:37.617007Z",
     "start_time": "2024-12-03T15:31:28.754122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "train_and_evaluate(model, 25)"
   ],
   "id": "eb2830c0e98d1679",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.6626\n",
      "Epoch 2, Loss: 1.3536\n",
      "Epoch 3, Loss: 1.2238\n",
      "Epoch 4, Loss: 1.1379\n",
      "Epoch 5, Loss: 1.0740\n",
      "Epoch 6, Loss: 1.0213\n",
      "Epoch 7, Loss: 0.9191\n",
      "Epoch 8, Loss: 0.8979\n",
      "Epoch 9, Loss: 0.8777\n",
      "Epoch 10, Loss: 0.8601\n",
      "Epoch 11, Loss: 0.8452\n",
      "Epoch 12, Loss: 0.8303\n",
      "Epoch 13, Loss: 0.7929\n",
      "Epoch 14, Loss: 0.7857\n",
      "Epoch 15, Loss: 0.7803\n",
      "Epoch 16, Loss: 0.7756\n",
      "Epoch 17, Loss: 0.7709\n",
      "Epoch 18, Loss: 0.7663\n",
      "Epoch 19, Loss: 0.7536\n",
      "Epoch 20, Loss: 0.7518\n",
      "Epoch 21, Loss: 0.7501\n",
      "Epoch 22, Loss: 0.7492\n",
      "Epoch 23, Loss: 0.7472\n",
      "Epoch 24, Loss: 0.7457\n",
      "Epoch 25, Loss: 0.7420\n",
      "Accuracy: 65.36%\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T15:05:23.167859Z",
     "start_time": "2024-12-03T15:04:34.397664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MNISTViT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTViT, self).__init__()\n",
    "        # Load a pretrained ViT model\n",
    "        self.vit = vit_b_16(pretrained=True)\n",
    "\n",
    "        # Modify the classifier to match the number of classes in MNIST\n",
    "        self.vit.heads = nn.Linear(self.vit.hidden_dim, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vit(x)\n",
    "\n",
    "# Instantiate and train\n",
    "model = MNISTViT()\n",
    "model.to(device)\n",
    "train_and_evaluate(model, 10)"
   ],
   "id": "29f4b9869a1c9bca",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /Users/nicolae.mogage/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
      "10.4%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 14\u001B[0m\n\u001B[1;32m     11\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvit(x)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Instantiate and train\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mMNISTViT\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m model\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     16\u001B[0m train_and_evaluate(model, \u001B[38;5;241m10\u001B[39m)\n",
      "Cell \u001B[0;32mIn[36], line 5\u001B[0m, in \u001B[0;36mMNISTViT.__init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28msuper\u001B[39m(MNISTViT, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Load a pretrained ViT model\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvit \u001B[38;5;241m=\u001B[39m \u001B[43mvit_b_16\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Modify the classifier to match the number of classes in MNIST\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvit\u001B[38;5;241m.\u001B[39mheads \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvit\u001B[38;5;241m.\u001B[39mhidden_dim, \u001B[38;5;241m10\u001B[39m)\n",
      "File \u001B[0;32m~/Uni/Masters/Semester 1/Advanced Methods in Data Analysis/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:142\u001B[0m, in \u001B[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    135\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    136\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msequence_to_str(\u001B[38;5;28mtuple\u001B[39m(keyword_only_kwargs\u001B[38;5;241m.\u001B[39mkeys()),\u001B[38;5;250m \u001B[39mseparate_last\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mand \u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m as positional \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    137\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    138\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    139\u001B[0m     )\n\u001B[1;32m    140\u001B[0m     kwargs\u001B[38;5;241m.\u001B[39mupdate(keyword_only_kwargs)\n\u001B[0;32m--> 142\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Uni/Masters/Semester 1/Advanced Methods in Data Analysis/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:228\u001B[0m, in \u001B[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    225\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m kwargs[pretrained_param]\n\u001B[1;32m    226\u001B[0m     kwargs[weights_param] \u001B[38;5;241m=\u001B[39m default_weights_arg\n\u001B[0;32m--> 228\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbuilder\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Uni/Masters/Semester 1/Advanced Methods in Data Analysis/.venv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py:641\u001B[0m, in \u001B[0;36mvit_b_16\u001B[0;34m(weights, progress, **kwargs)\u001B[0m\n\u001B[1;32m    622\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    623\u001B[0m \u001B[38;5;124;03mConstructs a vit_b_16 architecture from\u001B[39;00m\n\u001B[1;32m    624\u001B[0m \u001B[38;5;124;03m`An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>`_.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    637\u001B[0m \u001B[38;5;124;03m    :members:\u001B[39;00m\n\u001B[1;32m    638\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    639\u001B[0m weights \u001B[38;5;241m=\u001B[39m ViT_B_16_Weights\u001B[38;5;241m.\u001B[39mverify(weights)\n\u001B[0;32m--> 641\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_vision_transformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    642\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    643\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m12\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    644\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m12\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    645\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m768\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    646\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmlp_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3072\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    647\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    648\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprogress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Uni/Masters/Semester 1/Advanced Methods in Data Analysis/.venv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py:335\u001B[0m, in \u001B[0;36m_vision_transformer\u001B[0;34m(patch_size, num_layers, num_heads, hidden_dim, mlp_dim, weights, progress, **kwargs)\u001B[0m\n\u001B[1;32m    324\u001B[0m model \u001B[38;5;241m=\u001B[39m VisionTransformer(\n\u001B[1;32m    325\u001B[0m     image_size\u001B[38;5;241m=\u001B[39mimage_size,\n\u001B[1;32m    326\u001B[0m     patch_size\u001B[38;5;241m=\u001B[39mpatch_size,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    331\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    332\u001B[0m )\n\u001B[1;32m    334\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m weights:\n\u001B[0;32m--> 335\u001B[0m     model\u001B[38;5;241m.\u001B[39mload_state_dict(\u001B[43mweights\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprogress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m)\n\u001B[1;32m    337\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m~/Uni/Masters/Semester 1/Advanced Methods in Data Analysis/.venv/lib/python3.12/site-packages/torchvision/models/_api.py:90\u001B[0m, in \u001B[0;36mWeightsEnum.get_state_dict\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_state_dict\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Mapping[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[0;32m---> 90\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_state_dict_from_url\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Uni/Masters/Semester 1/Advanced Methods in Data Analysis/.venv/lib/python3.12/site-packages/torch/hub.py:867\u001B[0m, in \u001B[0;36mload_state_dict_from_url\u001B[0;34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001B[0m\n\u001B[1;32m    865\u001B[0m         r \u001B[38;5;241m=\u001B[39m HASH_REGEX\u001B[38;5;241m.\u001B[39msearch(filename)  \u001B[38;5;66;03m# r is Optional[Match[str]]\u001B[39;00m\n\u001B[1;32m    866\u001B[0m         hash_prefix \u001B[38;5;241m=\u001B[39m r\u001B[38;5;241m.\u001B[39mgroup(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m r \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 867\u001B[0m     \u001B[43mdownload_url_to_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcached_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhash_prefix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprogress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    869\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _is_legacy_zip_format(cached_file):\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n",
      "File \u001B[0;32m~/Uni/Masters/Semester 1/Advanced Methods in Data Analysis/.venv/lib/python3.12/site-packages/torch/hub.py:744\u001B[0m, in \u001B[0;36mdownload_url_to_file\u001B[0;34m(url, dst, hash_prefix, progress)\u001B[0m\n\u001B[1;32m    736\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tqdm(\n\u001B[1;32m    737\u001B[0m     total\u001B[38;5;241m=\u001B[39mfile_size,\n\u001B[1;32m    738\u001B[0m     disable\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m progress,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    741\u001B[0m     unit_divisor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1024\u001B[39m,\n\u001B[1;32m    742\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[1;32m    743\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 744\u001B[0m         buffer \u001B[38;5;241m=\u001B[39m \u001B[43mu\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mREAD_DATA_CHUNK\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    745\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(buffer) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    746\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/http/client.py:479\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    476\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength:\n\u001B[1;32m    477\u001B[0m     \u001B[38;5;66;03m# clip the read to the \"end of response\"\u001B[39;00m\n\u001B[1;32m    478\u001B[0m     amt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength\n\u001B[0;32m--> 479\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    480\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m s \u001B[38;5;129;01mand\u001B[39;00m amt:\n\u001B[1;32m    481\u001B[0m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[1;32m    482\u001B[0m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_conn()\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/socket.py:720\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    718\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    719\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 720\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    721\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    722\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/ssl.py:1251\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1247\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1248\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1249\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1250\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1252\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1253\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/ssl.py:1103\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1101\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1102\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1103\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1104\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1105\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:09:22.176320Z",
     "start_time": "2024-12-03T14:07:15.125696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CNNTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNTransformer, self).__init__()\n",
    "        # CNN feature extractor\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Transformer encoder\n",
    "        self.embedding_dim = 64 * 7 * 7  # Output size of CNN layers after pooling\n",
    "        self.embedding = nn.Linear(self.embedding_dim, 128)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=4)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN feature extraction\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "\n",
    "        # Flatten and transform to embedding\n",
    "        x = x.view(x.size(0), -1)  # Flattening into [batch_size, embedding_dim]\n",
    "        x = self.embedding(x).unsqueeze(1)  # [batch_size, seq_len=1, embedding_dim=128]\n",
    "\n",
    "        # Transformer and classification\n",
    "        x = self.transformer(x)  # Transformer expects [batch_size, seq_len, embedding_dim]\n",
    "        x = x.mean(dim=1)  # Aggregate sequence dimension (global average pooling)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Train and evaluate\n",
    "model = CNNTransformer()\n",
    "train_and_evaluate(model, 10)"
   ],
   "id": "7433bad584738b36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2780\n",
      "Epoch 2, Loss: 0.0693\n",
      "Epoch 3, Loss: 0.0522\n",
      "Epoch 4, Loss: 0.0418\n",
      "Epoch 5, Loss: 0.0382\n",
      "Epoch 6, Loss: 0.0307\n",
      "Epoch 7, Loss: 0.0129\n",
      "Epoch 8, Loss: 0.0090\n",
      "Epoch 9, Loss: 0.0074\n",
      "Epoch 10, Loss: 0.0071\n",
      "Accuracy: 99.11%\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T15:39:41.046273Z",
     "start_time": "2024-12-03T15:35:20.031696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CNNTransformer(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNNTransformer, self).__init__()\n",
    "\n",
    "        # CNN feature extractor\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Transformer encoder\n",
    "        self.embedding_dim = 64 * 8 * 8  # Output size of CNN layers after pooling\n",
    "        self.embedding = nn.Linear(self.embedding_dim, 128)  # Project to Transformer dim\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=4, dim_feedforward=256)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN feature extraction\n",
    "        x = self.pool(self.relu(self.conv1(x)))  # [batch_size, 32, 16, 16]\n",
    "        x = self.pool(self.relu(self.conv2(x)))  # [batch_size, 64, 8, 8]\n",
    "\n",
    "        # Flatten and transform to embedding\n",
    "        x = x.view(x.size(0), -1)  # Flatten to [batch_size, embedding_dim]\n",
    "        x = self.embedding(x).unsqueeze(1)  # [batch_size, seq_len=1, embedding_dim=128]\n",
    "\n",
    "        # Transformer encoder\n",
    "        x = self.transformer(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        x = x.mean(dim=1)  # Aggregate sequence dimension (global average pooling)\n",
    "\n",
    "        # Classification\n",
    "        return self.fc(x)\n",
    "\n",
    "model = CNNTransformer()\n",
    "train_and_evaluate(model, 25)"
   ],
   "id": "7d7e8512618b0afe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.2988\n",
      "Epoch 2, Loss: 0.8997\n",
      "Epoch 3, Loss: 0.7632\n",
      "Epoch 4, Loss: 0.6596\n",
      "Epoch 5, Loss: 0.5803\n",
      "Epoch 6, Loss: 0.5122\n",
      "Epoch 7, Loss: 0.4459\n",
      "Epoch 8, Loss: 0.3847\n",
      "Epoch 9, Loss: 0.3328\n",
      "Epoch 10, Loss: 0.2929\n",
      "Epoch 11, Loss: 0.2520\n",
      "Epoch 12, Loss: 0.2163\n",
      "Epoch 13, Loss: 0.1905\n",
      "Epoch 14, Loss: 0.1803\n",
      "Epoch 15, Loss: 0.1619\n",
      "Epoch 16, Loss: 0.1454\n",
      "Epoch 17, Loss: 0.1373\n",
      "Epoch 18, Loss: 0.1259\n",
      "Epoch 19, Loss: 0.1241\n",
      "Epoch 20, Loss: 0.1119\n",
      "Epoch 21, Loss: 0.1187\n",
      "Epoch 22, Loss: 0.1041\n",
      "Epoch 23, Loss: 0.0990\n",
      "Epoch 24, Loss: 0.1040\n",
      "Epoch 25, Loss: 0.1071\n",
      "Accuracy: 70.81%\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T15:12:00.439277Z",
     "start_time": "2024-12-03T15:11:42.028459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Convert datasets to numpy arrays\n",
    "X_train = train_dataset.data.numpy().reshape(-1, 28 * 28)  # Flatten 28x28 images into 1D arrays\n",
    "y_train = train_dataset.targets.numpy()\n",
    "X_test = test_dataset.data.numpy().reshape(-1, 28 * 28)\n",
    "y_test = test_dataset.targets.numpy()\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ],
   "id": "de31e25bcc995cea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.91%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.96      0.96      0.96      1032\n",
      "           3       0.96      0.96      0.96      1010\n",
      "           4       0.98      0.97      0.97       982\n",
      "           5       0.97      0.96      0.97       892\n",
      "           6       0.98      0.97      0.98       958\n",
      "           7       0.97      0.96      0.97      1028\n",
      "           8       0.96      0.96      0.96       974\n",
      "           9       0.96      0.96      0.96      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T11:06:05.293463Z",
     "start_time": "2024-12-04T11:01:48.743188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# CIFAR-10 Dataset and DataLoader\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# Load Pretrained ResNet Model\n",
    "class ResNetCIFAR10(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNetCIFAR10, self).__init__()\n",
    "        # Load a pretrained ResNet18 model\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "        # Replace the final fully connected layer to match CIFAR-10 classes\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = ResNetCIFAR10().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n",
    "# Train and evaluate the model\n",
    "train(model, train_loader, criterion, optimizer, epochs=15)\n",
    "evaluate(model, test_loader)"
   ],
   "id": "400aa8fa2d52d94e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/10], Loss: 0.9558\n",
      "Epoch [2/10], Loss: 0.6538\n",
      "Epoch [3/10], Loss: 0.5331\n",
      "Epoch [4/10], Loss: 0.4252\n",
      "Epoch [5/10], Loss: 0.3339\n",
      "Epoch [6/10], Loss: 0.2649\n",
      "Epoch [7/10], Loss: 0.2458\n",
      "Epoch [8/10], Loss: 0.1864\n",
      "Epoch [9/10], Loss: 0.1485\n",
      "Epoch [10/10], Loss: 0.1212\n",
      "Test Accuracy: 79.88%\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T11:48:46.119357Z",
     "start_time": "2024-12-04T11:48:44.288436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# CIFAR-10 Dataset and DataLoader\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# Load Pretrained ViT Model and Adapt It for CIFAR-10\n",
    "class ViTLargeCIFAR10(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ViTLargeCIFAR10, self).__init__()\n",
    "        # Load pretrained ViT-B-32\n",
    "        self.vit = models.vit_b_32(weights=models.ViT_B_32_Weights.DEFAULT)\n",
    "        self.vit.heads = nn.Linear(self.vit.hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        return self.vit(x)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = ViTLargeCIFAR10().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n",
    "# Train and evaluate the model\n",
    "train(model, train_loader, criterion, optimizer, epochs=10)\n",
    "evaluate(model, test_loader)"
   ],
   "id": "d7dbec812a812e6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 82\u001B[0m\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;241m100\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39mcorrect\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39mtotal\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     81\u001B[0m \u001B[38;5;66;03m# Train and evaluate the model\u001B[39;00m\n\u001B[0;32m---> 82\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     83\u001B[0m evaluate(model, test_loader)\n",
      "Cell \u001B[0;32mIn[3], line 59\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, criterion, optimizer, epochs)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n\u001B[1;32m     58\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 59\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     62\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/Uni/Masters/Semester 1/Advanced Methods in Data Analysis/.venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    580\u001B[0m     )\n\u001B[0;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Uni/Masters/Semester 1/Advanced Methods in Data Analysis/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Uni/Masters/Semester 1/Advanced Methods in Data Analysis/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    826\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    827\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "165cf2659833b4b7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
